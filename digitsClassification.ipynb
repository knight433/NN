{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import neuralNetwork \n",
    "from os.path  import join\n",
    "from datasetLoad import MnistDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {np.shape(self.weight_matrix)}') #debugging\n",
    "        values = np.dot(input_matrix,self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        # print(self.neuronValues) #debugging\n",
    "        return self.neuronValues\n",
    "    \n",
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,):\n",
    "        inputNumber = int(input('enter the input parameters: ')) # input nodes\n",
    "        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = layers.neuronValue\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "    \n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,parameters):\n",
    "        ind = 0\n",
    "        inputNumber = parameters[ind] # input nodes\n",
    "        ind += 1\n",
    "        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\n",
    "        ind += 1\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = parameters[ind]\n",
    "            ind += 1 \n",
    "            numberOfNeurons = parameters[ind]\n",
    "            ind += 1\n",
    "            activation = parameters[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        print('here') #debugging\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            print(f'wei = {np.shape(upWeights)}') #debugging\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        print(f'new = {np.shape(newValues)}') #debugging\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        print(f'cur_gra = {np.shape(cur_gradient)}') #debugging\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        print(f'weight = {np.shape(weights)}') #debugging\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'C:\\programs\\projects\\NN\\input'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# print(x_test[1],y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput parameters = 784\\nnumber of layers = 3\\n\\nlayer 1:\\nnumber of input: 784\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 2:\\nnumber of input: 16\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 3:\\nnumber of input: 16\\nnumber of neurons: 10\\nEnter the activation funtion: sigmoid\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)  \n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)     \n",
    "\n",
    "testInputs = [784 ,3,784,16,'relu',16,16,'relu',16,10,'sigmoid']\n",
    "model = ModelTest(testInputs)\n",
    "\n",
    "'''\n",
    "input parameters = 784\n",
    "number of layers = 3\n",
    "\n",
    "layer 1:\n",
    "number of input: 784\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 2:\n",
    "number of input: 16\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 3:\n",
    "number of input: 16\n",
    "number of neurons: 10\n",
    "Enter the activation funtion: sigmoid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "shapeofIN = (1, 784), shapeofWI = (784, 16)\n",
      "shapeofIN = (1, 16), shapeofWI = (16, 16)\n",
      "shapeofIN = (1, 16), shapeofWI = (16, 10)\n",
      "wei = (16, 16)\n",
      "new = (1, 16)\n",
      "cur_gra = [[-4.55607717e+05]\n",
      " [-2.39228741e+05]\n",
      " [-9.50213407e+05]\n",
      " [-1.67630505e+01]\n",
      " [-1.00377247e+02]\n",
      " [-8.74004025e+05]\n",
      " [-3.31548240e+05]\n",
      " [-7.46466287e+05]\n",
      " [-1.70514483e+04]\n",
      " [ 1.06842581e+01]\n",
      " [-1.09262789e+02]\n",
      " [-5.31437349e+05]\n",
      " [ 6.50431649e+00]\n",
      " [-4.22379704e+01]\n",
      " [-3.94271890e+01]\n",
      " [-8.49508759e+05]]\n",
      "weight = [[ 0.2016439  -0.12789001  0.12207099 ...  0.70237823 -0.73677803\n",
      "  -0.93113891]\n",
      " [-0.24463978 -0.26459953  0.45020729 ... -0.02833805  0.57157274\n",
      "   0.63737724]\n",
      " [-0.0929164  -0.53742713  0.88112299 ...  0.44064022  0.25186576\n",
      "  -0.08356085]\n",
      " ...\n",
      " [ 0.33649506  0.30422283 -0.31868325 ... -0.77895707 -0.57609631\n",
      "  -0.60621024]\n",
      " [-0.68505383  0.65114995  0.00712311 ...  0.89653097 -0.16972521\n",
      "  -0.34070558]\n",
      " [-0.246392   -0.91191439  0.69724719 ...  0.78749062 -0.17976716\n",
      "   0.81363246]]\n",
      "here\n",
      "shapeofIN = (1, 784), shapeofWI = (784, 16)\n",
      "shapeofIN = (1, 16), shapeofWI = (784, 16)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,16) and (784,16) not aligned: 16 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x_train_flat, y_train):\n\u001b[0;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 219\u001b[0m, in \u001b[0;36mModelTest.train\u001b[1;34m(self, data, label, learningRate)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m,data,label,learningRate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#debugging\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     pridiction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpridict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mForTraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     reqArr \u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumberOfLabels)])\n\u001b[0;32m    221\u001b[0m     reqArr[label] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 196\u001b[0m, in \u001b[0;36mModelTest.pridict\u001b[1;34m(self, data, ForTraining)\u001b[0m\n\u001b[0;32m    193\u001b[0m arr \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlistOfHiddenLayers:\n\u001b[1;32m--> 196\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ForTraining:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m, in \u001b[0;36mlayers.forward\u001b[1;34m(self, input_matrix)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_matrix):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshapeofIN = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(input_matrix)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shapeofWI = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_matrix)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#debugging\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_matrix\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneuronValues \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_activation(values)\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# print(self.neuronValues) #debugging\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,16) and (784,16) not aligned: 16 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_train_flat, y_train):\n",
    "    x = np.array([x])\n",
    "    model.train(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 312.70795421    0.          659.66035991    0.            0.\n",
      "   202.32734049    0.            0.            0.            0.\n",
      "   796.61605486 1666.14526113 1366.73697261  405.05207695   63.8154843\n",
      "     0.        ]]\n",
      "[[1244.67081574    0.           50.4632191     0.            0.\n",
      "     0.          389.93838025    0.            0.          791.08213188\n",
      "  1399.00574794    0.            0.          261.57572019  914.36981593\n",
      "     0.        ]]\n",
      "[[0.00000000e+000 1.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 1.70889831e-309 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 9, 'confidence': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([x_train_flat[0]])\n",
    "model.pridict(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
