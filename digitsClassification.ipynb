{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import neuralNetwork \n",
    "from os.path  import join\n",
    "from datasetLoad import MnistDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#input layer\\nclass inputLayer:\\n\\n    def __init__(self, number_of_input): \\n        self.number_of_input = number_of_input\\n    \\n    #to add the vlaues in the network \\n    def use(self, input_matrix):\\n        if isinstance(input_matrix,np.ndarray):\\n            self.out_matrix = input_matrix\\n            return input_matrix\\n        else:\\n            self.out_matrix = np.array(input_matrix)\\n            return self.out_matrix\\n    \\n    def fetchInputValues(self):\\n        return self.out_matrix\\n\\n#hidden and output layers\\nclass layers:\\n    \\n    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\\n        low, high = -1, 1\\n        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\\n        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\\n        self.activation = activation\\n\\n    def apply_activation(self, values):\\n        if self.activation == 'relu':\\n            return np.maximum(0, values)\\n        elif self.activation == 'sigmoid':\\n            return 1 / (1 + np.exp(-values))\\n        elif self.activation == 'tanh':\\n            return np.tanh(values)\\n        elif self.activation == 'softmax':\\n            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\\n            return e_x / np.sum(e_x, axis=-1, keepdims=True)\\n        else:\\n            return values  # No activation (identity)\\n\\n    def compute_derivative(self, values):\\n        if self.activation == 'relu':\\n            return (values > 0).astype(float)\\n        elif self.activation == 'sigmoid':\\n            sigmoid_output = self.apply_activation(values)\\n            return sigmoid_output * (1 - sigmoid_output)\\n        elif self.activation == 'tanh':\\n            tanh_output = self.apply_activation(values)\\n            return 1 - tanh_output**2\\n        elif self.activation == 'softmax':\\n            softmax_output = self.apply_activation(values)\\n            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\\n        else:\\n            return np.ones_like(values)  # Derivative of the identity function is 1\\n\\n    def forward(self, input_matrix):\\n        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {np.shape(self.weight_matrix)}') #debugging\\n        values = np.dot(input_matrix,self.weight_matrix) + self.bias\\n        self.neuronValues =  self.apply_activation(values)\\n        # print(self.neuronValues) #debugging\\n        return self.neuronValues\\n    \\nclass Model:\\n\\n    #to intiallize the model\\n    def __init__(self,):\\n        inputNumber = int(input('enter the input parameters: ')) # input nodes\\n        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\\n\\n        self.listOfHiddenLayers = []\\n        self.inLayerObj = inputLayer(inputNumber)\\n\\n        for i in range(self.numberOfLayers):\\n            number_of_input = int(input('number of input: '))\\n            numberOfNeurons = int(input('number of neurons: '))\\n            activation = input('Enter the activation funtion: ')\\n            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\\n\\n            self.listOfHiddenLayers.append(hidden_layer_obj)\\n            \\n            if i == self.numberOfLayers-1:\\n                self.numberOfLabels = numberOfNeurons\\n\\n    def pridict(self,data,ForTraining = False):\\n\\n        arr = data\\n        \\n        for layer in self.listOfHiddenLayers:\\n            arr = layer.forward(arr)\\n\\n        if ForTraining:\\n            return arr\\n\\n        pridiction_value = -1\\n        pridiction = None\\n\\n        for i,obj in enumerate(arr[0]):\\n            if pridiction_value < obj:\\n                pridiction = {'label' : i, 'confidence' : obj}\\n        \\n        return pridiction\\n    \\n\\n    def view_weights(self):\\n        for layer in self.listOfHiddenLayers:\\n            print(f'weights \\n - {layer.weight_matrix}')\\n            print(f'bias - {layer.bias}')\\n\\n    #backpropagation \\n    def train(self,data,label,learningRate=0.1):\\n        \\n        pridiction = self.pridict(data,ForTraining = True)\\n        reqArr =np.array([0 for i in range(self.numberOfLabels)])\\n        reqArr[label] = 1\\n        costFuntion = sum((pridiction - reqArr)**2)\\n\\n        pridiction = pridiction - reqArr\\n        i = self.numberOfLayers - 1\\n\\n        layers = self.listOfHiddenLayers[i]\\n        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\\n        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\\n        layers.weight_matrix = pr.T\\n        layers.bias = layers.bias - learningRate*(gradient)\\n        # print(layers.bias) #debugging\\n        i -= 1\\n\\n        while i:\\n            #to get gradient \\n            layers = self.listOfHiddenLayers[i]\\n            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\\n            newValues = layers.neuronValues\\n            # print(f'newValues = {i} - {newValues}') #debugging\\n            derValues = layers.compute_derivative(newValues)\\n            cur_gradient = np.multiply(derValues.T,pre_w)\\n            pr = np.dot(gradient,cur_gradient.T)\\n            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n            \\n            #updating weights\\n            weights = layers.weight_matrix\\n            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n            layers.weight_matrix = upWeights\\n            gradient = pr\\n            \\n            #updating the bias \\n            layers.bias = layers.bias - learningRate*(pr)\\n            # print(layers.bias) #debugging\\n\\n            i -= 1\\n\\n        #for input layer - first layer value\\n        layer = self.listOfHiddenLayers[0]\\n        pre_w = self.listOfHiddenLayers[1].weight_matrix\\n        newValues = layers.neuronValue\\n        derValues = layers.compute_derivative(newValues)\\n        cur_gradient = np.dot(pre_w,gradient.T)\\n        pr = np.multiply(derValues,cur_gradient.T)\\n        prevNuronValues = data\\n        weights = layer.weight_matrix\\n        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n        layers.weight_matrix = upWeights.T\\n        layers.bias = layers.bias - learningRate*(pr)\\n    \\nclass ModelTest:\\n\\n    #to intiallize the model\\n    def __init__(self,parameters):\\n        ind = 0\\n        inputNumber = parameters[ind] # input nodes\\n        ind += 1\\n        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\\n        ind += 1\\n        self.listOfHiddenLayers = []\\n        self.inLayerObj = inputLayer(inputNumber)\\n\\n        for i in range(self.numberOfLayers):\\n            number_of_input = parameters[ind]\\n            ind += 1 \\n            numberOfNeurons = parameters[ind]\\n            ind += 1\\n            activation = parameters[ind]\\n            ind += 1\\n            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\\n\\n            self.listOfHiddenLayers.append(hidden_layer_obj)\\n            \\n            if i == self.numberOfLayers-1:\\n                self.numberOfLabels = numberOfNeurons\\n\\n    def pridict(self,data,ForTraining = False):\\n\\n        arr = data\\n        \\n        for layer in self.listOfHiddenLayers:\\n            arr = layer.forward(arr)\\n\\n        if ForTraining:\\n            return arr\\n\\n        pridiction_value = -1\\n        pridiction = None\\n\\n        for i,obj in enumerate(arr[0]):\\n            if pridiction_value < obj:\\n                pridiction = {'label' : i, 'confidence' : obj}\\n        \\n        return pridiction\\n    \\n\\n    def view_weights(self):\\n        for layer in self.listOfHiddenLayers:\\n            print(f'weights \\n - {layer.weight_matrix}')\\n            print(f'bias - {layer.bias}')\\n\\n    #backpropagation \\n    def train(self,data,label,learningRate=0.1):\\n        print('here') #debugging\\n        pridiction = self.pridict(data,ForTraining = True)\\n        reqArr =np.array([0 for i in range(self.numberOfLabels)])\\n        reqArr[label] = 1\\n        costFuntion = sum((pridiction - reqArr)**2)\\n\\n        pridiction = pridiction - reqArr\\n        i = self.numberOfLayers - 1\\n        layers = self.listOfHiddenLayers[i]\\n        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\\n        # print(f'grad{i} = {gradient.shape}') #debugging\\n        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n        # print(f'pre{i} = {prevNuronValues.shape}') #debugging\\n        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\\n        print(f'weight{i} = {pr.T.shape}') #debugging\\n        layers.weight_matrix = pr.T\\n        layers.bias = layers.bias - learningRate*(gradient)\\n        # print(layers.bias) #debugging\\n        i -= 1\\n\\n        while i:\\n            #to get gradient \\n            layers = self.listOfHiddenLayers[i]\\n            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\\n            newValues = layers.neuronValues\\n            # print(f'newValues = {i} - {newValues}') #debugging\\n            derValues = layers.compute_derivative(newValues)\\n            # print(f'der{i} = {derValues.shape}') #debugging\\n            cur_gradient = np.multiply(derValues.T,pre_w)\\n            # print(f'grad{i} = {cur_gradient.shape}') #debugging\\n            pr = np.dot(gradient,cur_gradient.T)\\n            # print(f'pr{i} = {pr.shape}') #debugging\\n            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n            # print(f'prevNu{i} = {prevNuronValues.shape}') #debugging\\n            \\n            #updating weights\\n            weights = layers.weight_matrix\\n            # print(f'hidden{i} - {np.shape(weights)}') #debugging\\n            upWeights = weights.T - learningRate*(np.dot(prevNuronValues.T,pr))\\n            # print(f'wei{i} = {np.shape(upWeights)}') #debugging\\n            layers.weight_matrix = upWeights\\n            gradient = pr\\n            \\n            #updating the bias \\n            layers.bias = layers.bias - learningRate*(pr)\\n            # print(layers.bias) #debugging\\n            print(f'weight{i} = {layer.view_weights}') #debugging\\n\\n            i -= 1\\n\\n        #for input layer - first layer value\\n        layer = self.listOfHiddenLayers[0]\\n        pre_w = self.listOfHiddenLayers[1].weight_matrix\\n        newValues = self.listOfHiddenLayers[1].neuronValues\\n        # print(f'new = {np.shape(newValues)}') #debugging\\n        derValues = layers.compute_derivative(newValues)\\n        cur_gradient = np.dot(pre_w,gradient.T)\\n        # print(f'cur_gra = {np.shape(cur_gradient)}') #debugging\\n        pr = np.multiply(derValues,cur_gradient.T)\\n        prevNuronValues = data\\n        weights = layer.weight_matrix\\n        # print(f'weight = {np.shape(weights)}') #debugging\\n        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n        layers.weight_matrix = upWeights.T\\n        print(f'up{i} = {layer.view_weights}') #debugging\\n        layers.bias = layers.bias - learningRate*(pr)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {np.shape(self.weight_matrix)}') #debugging\n",
    "        values = np.dot(input_matrix,self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        # print(self.neuronValues) #debugging\n",
    "        return self.neuronValues\n",
    "    \n",
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,):\n",
    "        inputNumber = int(input('enter the input parameters: ')) # input nodes\n",
    "        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = layers.neuronValue\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "    \n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,parameters):\n",
    "        ind = 0\n",
    "        inputNumber = parameters[ind] # input nodes\n",
    "        ind += 1\n",
    "        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\n",
    "        ind += 1\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = parameters[ind]\n",
    "            ind += 1 \n",
    "            numberOfNeurons = parameters[ind]\n",
    "            ind += 1\n",
    "            activation = parameters[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        print('here') #debugging\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        # print(f'grad{i} = {gradient.shape}') #debugging\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        # print(f'pre{i} = {prevNuronValues.shape}') #debugging\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        print(f'weight{i} = {pr.T.shape}') #debugging\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            # print(f'der{i} = {derValues.shape}') #debugging\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            # print(f'grad{i} = {cur_gradient.shape}') #debugging\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            # print(f'pr{i} = {pr.shape}') #debugging\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            # print(f'prevNu{i} = {prevNuronValues.shape}') #debugging\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            # print(f'hidden{i} - {np.shape(weights)}') #debugging\n",
    "            upWeights = weights.T - learningRate*(np.dot(prevNuronValues.T,pr))\n",
    "            # print(f'wei{i} = {np.shape(upWeights)}') #debugging\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "            print(f'weight{i} = {layer.view_weights}') #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        # print(f'new = {np.shape(newValues)}') #debugging\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        # print(f'cur_gra = {np.shape(cur_gradient)}') #debugging\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        # print(f'weight = {np.shape(weights)}') #debugging\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        print(f'up{i} = {layer.view_weights}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons,layerno, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "        self.layerNo = layerno\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {self.weight_matrix.shape} layerNo = {self.layerNo}') #debugging\n",
    "        values = np.dot(input_matrix,self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        # print(self.neuronValues) #debugging\n",
    "        return self.neuronValues\n",
    "    \n",
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,):\n",
    "        inputNumber = int(input('enter the input parameters: ')) # input nodes\n",
    "        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "    \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "       \n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights.T\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layers = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layers.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "        \n",
    "    \n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,parameters):\n",
    "        ind = 0\n",
    "        inputNumber = parameters[ind] # input nodes\n",
    "        ind += 1\n",
    "        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\n",
    "        ind += 1\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = parameters[ind]\n",
    "            ind += 1 \n",
    "            numberOfNeurons = parameters[ind]\n",
    "            ind += 1\n",
    "            activation = parameters[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,i,activation)\n",
    "            print(hidden_layer_obj.weight_matrix.shape)\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "            \n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for i,layer in enumerate(self.listOfHiddenLayers):\n",
    "            print(f'layer{i+1}') \n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    def view_neuronValues(self):   \n",
    "        for i,layer in enumerate(self.listOfHiddenLayers):\n",
    "            print(f'layer{i+1}') \n",
    "            print(f'neuronValues \\n - {layer.neuronValues}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        # print(f'req arr - {reqArr}') #debugging\n",
    "\n",
    "        print(f'pred = {pridiction}') #debugging\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "        # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(f'grad = {gradient}') #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            # print(f'grad - {pr}') #debugging\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights.T\n",
    "            # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "            # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layers = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        # print(f'grad - {pr}') #debugging\n",
    "        prevNuronValues = data\n",
    "        weights = layers.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "        # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'C:\\programs\\projects\\NN\\input'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# print(x_test[1],y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 16)\n",
      "(16, 16)\n",
      "(16, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninput parameters = 784\\nnumber of layers = 3\\n\\nlayer 1:\\nnumber of input: 784\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 2:\\nnumber of input: 16\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 3:\\nnumber of input: 16\\nnumber of neurons: 10\\nEnter the activation funtion: sigmoid\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)/255  \n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)     \n",
    "\n",
    "testInputs = [784 ,3,784,16,'relu',16,16,'relu',16,10,'sigmoid']\n",
    "model = ModelTest(testInputs)\n",
    "\n",
    "'''\n",
    "input parameters = 784\n",
    "number of layers = 3\n",
    "\n",
    "layer 1:\n",
    "number of input: 784\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 2:\n",
    "number of input: 16\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 3:\n",
    "number of input: 16\n",
    "number of neurons: 10\n",
    "Enter the activation funtion: sigmoid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = [[1.31682017e-18 9.99999997e-01 3.89699941e-13 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99961735e-01 1.00000000e+00]]\n",
      "layer1\n",
      "neuronValues \n",
      " - [[ 0.          0.          3.12997168  5.28690096  9.03985486  3.23132037\n",
      "  19.64828554  4.24926767  0.          0.          3.77605531  4.09499052\n",
      "   0.          0.          0.          0.        ]]\n",
      "layer2\n",
      "neuronValues \n",
      " - [[22.83976792 19.85484559  6.31842221  0.          0.9645683   0.48994338\n",
      "   0.          0.          0.         19.82246016  8.81888101 24.46529089\n",
      "   0.         14.21822934 26.50244999  7.91290158]]\n",
      "layer3\n",
      "neuronValues \n",
      " - [[1.31682017e-18 9.99999997e-01 3.89699941e-13 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      "  9.99961735e-01 1.00000000e+00]]\n",
      "pred = [[1.73115296e-24 5.79436067e-13 1.70122150e-07 3.57133486e-20\n",
      "  3.51226513e-14 1.00000000e+00 1.74867845e-04 9.85990459e-23\n",
      "  1.49041591e-31 3.91890541e-14]]\n",
      "layer1\n",
      "neuronValues \n",
      " - [[ 0.          0.          4.90594401  0.         12.5122137   4.61771851\n",
      "   4.64519168  0.          0.          0.          4.26318562  5.67303139\n",
      "   0.          0.          0.          0.        ]]\n",
      "layer2\n",
      "neuronValues \n",
      " - [[38.43197449 15.74987094  0.          0.          0.          0.\n",
      "   0.          0.          0.         18.50018366 14.60161996 14.6227434\n",
      "   0.         16.05710705 22.11373303  9.50807679]]\n",
      "layer3\n",
      "neuronValues \n",
      " - [[1.73115296e-24 5.79436067e-13 1.70122150e-07 3.57133486e-20\n",
      "  3.51226513e-14 1.00000000e+00 1.74867845e-04 9.85990459e-23\n",
      "  1.49041591e-31 3.91890541e-14]]\n",
      "pred = [[9.99999994e-01 1.63031056e-09 7.56338482e-14 7.67785330e-19\n",
      "  8.06715684e-20 3.84610022e-10 2.39259417e-06 3.48418917e-12\n",
      "  4.31135400e-13 9.60652700e-01]]\n",
      "layer1\n",
      "neuronValues \n",
      " - [[ 0.46181763  0.28936452  7.29024387  0.          0.69080314 10.16567089\n",
      "   0.         11.99341077  2.82768445  0.          4.67322468  3.20671571\n",
      "   0.          0.          1.99559247  0.        ]]\n",
      "layer2\n",
      "neuronValues \n",
      " - [[18.21465908 10.85236693  0.24285452  0.          0.          5.0475232\n",
      "   0.          4.33231777  9.78981304  0.          8.349309   33.47086429\n",
      "   0.          0.          9.60307358 11.80828196]]\n",
      "layer3\n",
      "neuronValues \n",
      " - [[9.99999994e-01 1.63031056e-09 7.56338482e-14 7.67785330e-19\n",
      "  8.06715684e-20 3.84610022e-10 2.39259417e-06 3.48418917e-12\n",
      "  4.31135400e-13 9.60652700e-01]]\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_train_flat, y_train[:3]):\n",
    "    x = np.array([x])\n",
    "    # model.view_weights()\n",
    "    model.train(x, y)\n",
    "    model.view_neuronValues()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      "  0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      "  0.96862745 0.49803922 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      "  0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19215686\n",
      "  0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      "  0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      "  0.96862745 0.94509804 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      "  0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04313725\n",
      "  0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.1372549  0.94509804\n",
      "  0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      "  0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      "  0.58823529 0.10588235 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      "  0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.15294118 0.58039216\n",
      "  0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      "  0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.07058824 0.67058824\n",
      "  0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      "  0.31372549 0.03529412 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      "  0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.53333333 0.99215686\n",
      "  0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([x_train_flat[0]])\n",
    "# print(model.pridict(a,True))\n",
    "\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
