{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import neuralNetwork \n",
    "from os.path  import join\n",
    "from datasetLoad import MnistDataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#input layer\\nclass inputLayer:\\n\\n    def __init__(self, number_of_input): \\n        self.number_of_input = number_of_input\\n    \\n    #to add the vlaues in the network \\n    def use(self, input_matrix):\\n        if isinstance(input_matrix,np.ndarray):\\n            self.out_matrix = input_matrix\\n            return input_matrix\\n        else:\\n            self.out_matrix = np.array(input_matrix)\\n            return self.out_matrix\\n    \\n    def fetchInputValues(self):\\n        return self.out_matrix\\n\\n#hidden and output layers\\nclass layers:\\n    \\n    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\\n        low, high = -1, 1\\n        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\\n        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\\n        self.activation = activation\\n\\n    def apply_activation(self, values):\\n        if self.activation == 'relu':\\n            return np.maximum(0, values)\\n        elif self.activation == 'sigmoid':\\n            return 1 / (1 + np.exp(-values))\\n        elif self.activation == 'tanh':\\n            return np.tanh(values)\\n        elif self.activation == 'softmax':\\n            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\\n            return e_x / np.sum(e_x, axis=-1, keepdims=True)\\n        else:\\n            return values  # No activation (identity)\\n\\n    def compute_derivative(self, values):\\n        if self.activation == 'relu':\\n            return (values > 0).astype(float)\\n        elif self.activation == 'sigmoid':\\n            sigmoid_output = self.apply_activation(values)\\n            return sigmoid_output * (1 - sigmoid_output)\\n        elif self.activation == 'tanh':\\n            tanh_output = self.apply_activation(values)\\n            return 1 - tanh_output**2\\n        elif self.activation == 'softmax':\\n            softmax_output = self.apply_activation(values)\\n            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\\n        else:\\n            return np.ones_like(values)  # Derivative of the identity function is 1\\n\\n    def forward(self, input_matrix):\\n        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {np.shape(self.weight_matrix)}') #debugging\\n        values = np.dot(input_matrix,self.weight_matrix) + self.bias\\n        self.neuronValues =  self.apply_activation(values)\\n        # print(self.neuronValues) #debugging\\n        return self.neuronValues\\n    \\nclass Model:\\n\\n    #to intiallize the model\\n    def __init__(self,):\\n        inputNumber = int(input('enter the input parameters: ')) # input nodes\\n        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\\n\\n        self.listOfHiddenLayers = []\\n        self.inLayerObj = inputLayer(inputNumber)\\n\\n        for i in range(self.numberOfLayers):\\n            number_of_input = int(input('number of input: '))\\n            numberOfNeurons = int(input('number of neurons: '))\\n            activation = input('Enter the activation funtion: ')\\n            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\\n\\n            self.listOfHiddenLayers.append(hidden_layer_obj)\\n            \\n            if i == self.numberOfLayers-1:\\n                self.numberOfLabels = numberOfNeurons\\n\\n    def pridict(self,data,ForTraining = False):\\n\\n        arr = data\\n        \\n        for layer in self.listOfHiddenLayers:\\n            arr = layer.forward(arr)\\n\\n        if ForTraining:\\n            return arr\\n\\n        pridiction_value = -1\\n        pridiction = None\\n\\n        for i,obj in enumerate(arr[0]):\\n            if pridiction_value < obj:\\n                pridiction = {'label' : i, 'confidence' : obj}\\n        \\n        return pridiction\\n    \\n\\n    def view_weights(self):\\n        for layer in self.listOfHiddenLayers:\\n            print(f'weights \\n - {layer.weight_matrix}')\\n            print(f'bias - {layer.bias}')\\n\\n    #backpropagation \\n    def train(self,data,label,learningRate=0.1):\\n        \\n        pridiction = self.pridict(data,ForTraining = True)\\n        reqArr =np.array([0 for i in range(self.numberOfLabels)])\\n        reqArr[label] = 1\\n        costFuntion = sum((pridiction - reqArr)**2)\\n\\n        pridiction = pridiction - reqArr\\n        i = self.numberOfLayers - 1\\n\\n        layers = self.listOfHiddenLayers[i]\\n        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\\n        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\\n        layers.weight_matrix = pr.T\\n        layers.bias = layers.bias - learningRate*(gradient)\\n        # print(layers.bias) #debugging\\n        i -= 1\\n\\n        while i:\\n            #to get gradient \\n            layers = self.listOfHiddenLayers[i]\\n            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\\n            newValues = layers.neuronValues\\n            # print(f'newValues = {i} - {newValues}') #debugging\\n            derValues = layers.compute_derivative(newValues)\\n            cur_gradient = np.multiply(derValues.T,pre_w)\\n            pr = np.dot(gradient,cur_gradient.T)\\n            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n            \\n            #updating weights\\n            weights = layers.weight_matrix\\n            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n            layers.weight_matrix = upWeights\\n            gradient = pr\\n            \\n            #updating the bias \\n            layers.bias = layers.bias - learningRate*(pr)\\n            # print(layers.bias) #debugging\\n\\n            i -= 1\\n\\n        #for input layer - first layer value\\n        layer = self.listOfHiddenLayers[0]\\n        pre_w = self.listOfHiddenLayers[1].weight_matrix\\n        newValues = layers.neuronValue\\n        derValues = layers.compute_derivative(newValues)\\n        cur_gradient = np.dot(pre_w,gradient.T)\\n        pr = np.multiply(derValues,cur_gradient.T)\\n        prevNuronValues = data\\n        weights = layer.weight_matrix\\n        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n        layers.weight_matrix = upWeights.T\\n        layers.bias = layers.bias - learningRate*(pr)\\n    \\nclass ModelTest:\\n\\n    #to intiallize the model\\n    def __init__(self,parameters):\\n        ind = 0\\n        inputNumber = parameters[ind] # input nodes\\n        ind += 1\\n        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\\n        ind += 1\\n        self.listOfHiddenLayers = []\\n        self.inLayerObj = inputLayer(inputNumber)\\n\\n        for i in range(self.numberOfLayers):\\n            number_of_input = parameters[ind]\\n            ind += 1 \\n            numberOfNeurons = parameters[ind]\\n            ind += 1\\n            activation = parameters[ind]\\n            ind += 1\\n            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\\n\\n            self.listOfHiddenLayers.append(hidden_layer_obj)\\n            \\n            if i == self.numberOfLayers-1:\\n                self.numberOfLabels = numberOfNeurons\\n\\n    def pridict(self,data,ForTraining = False):\\n\\n        arr = data\\n        \\n        for layer in self.listOfHiddenLayers:\\n            arr = layer.forward(arr)\\n\\n        if ForTraining:\\n            return arr\\n\\n        pridiction_value = -1\\n        pridiction = None\\n\\n        for i,obj in enumerate(arr[0]):\\n            if pridiction_value < obj:\\n                pridiction = {'label' : i, 'confidence' : obj}\\n        \\n        return pridiction\\n    \\n\\n    def view_weights(self):\\n        for layer in self.listOfHiddenLayers:\\n            print(f'weights \\n - {layer.weight_matrix}')\\n            print(f'bias - {layer.bias}')\\n\\n    #backpropagation \\n    def train(self,data,label,learningRate=0.1):\\n        print('here') #debugging\\n        pridiction = self.pridict(data,ForTraining = True)\\n        reqArr =np.array([0 for i in range(self.numberOfLabels)])\\n        reqArr[label] = 1\\n        costFuntion = sum((pridiction - reqArr)**2)\\n\\n        pridiction = pridiction - reqArr\\n        i = self.numberOfLayers - 1\\n        layers = self.listOfHiddenLayers[i]\\n        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\\n        # print(f'grad{i} = {gradient.shape}') #debugging\\n        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n        # print(f'pre{i} = {prevNuronValues.shape}') #debugging\\n        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\\n        print(f'weight{i} = {pr.T.shape}') #debugging\\n        layers.weight_matrix = pr.T\\n        layers.bias = layers.bias - learningRate*(gradient)\\n        # print(layers.bias) #debugging\\n        i -= 1\\n\\n        while i:\\n            #to get gradient \\n            layers = self.listOfHiddenLayers[i]\\n            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\\n            newValues = layers.neuronValues\\n            # print(f'newValues = {i} - {newValues}') #debugging\\n            derValues = layers.compute_derivative(newValues)\\n            # print(f'der{i} = {derValues.shape}') #debugging\\n            cur_gradient = np.multiply(derValues.T,pre_w)\\n            # print(f'grad{i} = {cur_gradient.shape}') #debugging\\n            pr = np.dot(gradient,cur_gradient.T)\\n            # print(f'pr{i} = {pr.shape}') #debugging\\n            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\\n            # print(f'prevNu{i} = {prevNuronValues.shape}') #debugging\\n            \\n            #updating weights\\n            weights = layers.weight_matrix\\n            # print(f'hidden{i} - {np.shape(weights)}') #debugging\\n            upWeights = weights.T - learningRate*(np.dot(prevNuronValues.T,pr))\\n            # print(f'wei{i} = {np.shape(upWeights)}') #debugging\\n            layers.weight_matrix = upWeights\\n            gradient = pr\\n            \\n            #updating the bias \\n            layers.bias = layers.bias - learningRate*(pr)\\n            # print(layers.bias) #debugging\\n            print(f'weight{i} = {layer.view_weights}') #debugging\\n\\n            i -= 1\\n\\n        #for input layer - first layer value\\n        layer = self.listOfHiddenLayers[0]\\n        pre_w = self.listOfHiddenLayers[1].weight_matrix\\n        newValues = self.listOfHiddenLayers[1].neuronValues\\n        # print(f'new = {np.shape(newValues)}') #debugging\\n        derValues = layers.compute_derivative(newValues)\\n        cur_gradient = np.dot(pre_w,gradient.T)\\n        # print(f'cur_gra = {np.shape(cur_gradient)}') #debugging\\n        pr = np.multiply(derValues,cur_gradient.T)\\n        prevNuronValues = data\\n        weights = layer.weight_matrix\\n        # print(f'weight = {np.shape(weights)}') #debugging\\n        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\\n        layers.weight_matrix = upWeights.T\\n        print(f'up{i} = {layer.view_weights}') #debugging\\n        layers.bias = layers.bias - learningRate*(pr)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {np.shape(self.weight_matrix)}') #debugging\n",
    "        values = np.dot(input_matrix,self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        # print(self.neuronValues) #debugging\n",
    "        return self.neuronValues\n",
    "    \n",
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,):\n",
    "        inputNumber = int(input('enter the input parameters: ')) # input nodes\n",
    "        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = layers.neuronValue\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "    \n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,parameters):\n",
    "        ind = 0\n",
    "        inputNumber = parameters[ind] # input nodes\n",
    "        ind += 1\n",
    "        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\n",
    "        ind += 1\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = parameters[ind]\n",
    "            ind += 1 \n",
    "            numberOfNeurons = parameters[ind]\n",
    "            ind += 1\n",
    "            activation = parameters[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        print('here') #debugging\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        # print(f'grad{i} = {gradient.shape}') #debugging\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        # print(f'pre{i} = {prevNuronValues.shape}') #debugging\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        print(f'weight{i} = {pr.T.shape}') #debugging\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            # print(f'der{i} = {derValues.shape}') #debugging\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            # print(f'grad{i} = {cur_gradient.shape}') #debugging\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            # print(f'pr{i} = {pr.shape}') #debugging\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            # print(f'prevNu{i} = {prevNuronValues.shape}') #debugging\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            # print(f'hidden{i} - {np.shape(weights)}') #debugging\n",
    "            upWeights = weights.T - learningRate*(np.dot(prevNuronValues.T,pr))\n",
    "            # print(f'wei{i} = {np.shape(upWeights)}') #debugging\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "            print(f'weight{i} = {layer.view_weights}') #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        # print(f'new = {np.shape(newValues)}') #debugging\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        # print(f'cur_gra = {np.shape(cur_gradient)}') #debugging\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layer.weight_matrix\n",
    "        # print(f'weight = {np.shape(weights)}') #debugging\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        print(f'up{i} = {layer.view_weights}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons,layerno, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "        self.layerNo = layerno\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return values * (1 - values)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        # print(f'shapeofIN = {np.shape(input_matrix)}, shapeofWI = {self.weight_matrix.shape} layerNo = {self.layerNo}') #debugging\n",
    "        values = np.dot(input_matrix,self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        # print(self.neuronValues) #debugging\n",
    "        return self.neuronValues\n",
    "    \n",
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,):\n",
    "        inputNumber = int(input('enter the input parameters: ')) # input nodes\n",
    "        self.numberOfLayers = int(input('enter the number of layers: ')) #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "    \n",
    "    def train(self,data,label,learningRate=1):\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "       \n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights.T\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layers = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = data\n",
    "        weights = layers.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "        \n",
    "    \n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self,parameters):\n",
    "        ind = 0\n",
    "        inputNumber = parameters[ind] # input nodes\n",
    "        ind += 1\n",
    "        self.numberOfLayers = parameters[ind] #number of hidden layer and output layer\n",
    "        ind += 1\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = parameters[ind]\n",
    "            ind += 1 \n",
    "            numberOfNeurons = parameters[ind]\n",
    "            ind += 1\n",
    "            activation = parameters[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,i,activation)\n",
    "            print(hidden_layer_obj.weight_matrix.shape)\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = data\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "            \n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for i,layer in enumerate(self.listOfHiddenLayers):\n",
    "            print(f'layer{i+1}') \n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    def view_neuronValues(self):   \n",
    "        for i,layer in enumerate(self.listOfHiddenLayers):\n",
    "            print(f'layer{i+1}') \n",
    "            print(f'neuronValues \\n - {layer.neuronValues}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        # print(f'req arr - {reqArr}') #debugging\n",
    "        \n",
    "        print(f'pred = {pridiction}') #debugging\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "        print(f'grad{i} - {gradient}') #debugging\n",
    "        prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "        # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        # print(f'grad = {gradient}') #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = layers.compute_derivative(newValues)\n",
    "            cur_gradient = np.multiply(derValues.T,pre_w)\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            print(f'grad{i} - {pr}') #debugging\n",
    "            prevNuronValues = self.listOfHiddenLayers[i-1].neuronValues\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights.T\n",
    "            # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "            # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            # print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layers = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = self.listOfHiddenLayers[1].neuronValues\n",
    "        derValues = layers.compute_derivative(newValues)\n",
    "        cur_gradient = np.dot(pre_w,gradient.T)\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        print(f'grad{i} - {pr}') #debugging\n",
    "        prevNuronValues = data\n",
    "        weights = layers.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        # print(f'we{i}= {np.shape(layers.weight_matrix)}') #debugging\n",
    "        # print(f'ne{i}= {np.shape(layers.neuronValues)}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'C:\\programs\\projects\\NN\\input'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# print(x_test[1],y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 16)\n",
      "(16, 16)\n",
      "(16, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninput parameters = 784\\nnumber of layers = 3\\n\\nlayer 1:\\nnumber of input: 784\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 2:\\nnumber of input: 16\\nnumber of neurons: 16\\nEnter the activation funtion: relu\\n\\nlayer 3:\\nnumber of input: 16\\nnumber of neurons: 10\\nEnter the activation funtion: sigmoid\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the training and test images\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], -1)/255  \n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1)     \n",
    "\n",
    "testInputs = [784 ,3,784,16,'relu',16,16,'relu',16,10,'sigmoid']\n",
    "model = ModelTest(testInputs)\n",
    "\n",
    "'''\n",
    "input parameters = 784\n",
    "number of layers = 3\n",
    "\n",
    "layer 1:\n",
    "number of input: 784\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 2:\n",
    "number of input: 16\n",
    "number of neurons: 16\n",
    "Enter the activation funtion: relu\n",
    "\n",
    "layer 3:\n",
    "number of input: 16\n",
    "number of neurons: 10\n",
    "Enter the activation funtion: sigmoid\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = [[9.99994585e-01 4.41885801e-08 9.99968095e-01 1.46836414e-02\n",
      "  9.98864825e-01 1.00000000e+00 1.00000000e+00 1.72261224e-08\n",
      "  1.63410920e-17 9.79309207e-15]]\n",
      "grad2 - [[5.41464606e-06 1.95263052e-15 3.19029827e-05 2.12443394e-04\n",
      "  1.13259901e-03 5.62719939e-21 1.32265310e-11 2.96739288e-16\n",
      "  2.67031287e-34 9.59046523e-29]]\n",
      "grad1 - [[ 0.00000000e+00  0.00000000e+00  6.61971642e-04  0.00000000e+00\n",
      "  -1.26246331e-05  0.00000000e+00  5.71851761e-05  0.00000000e+00\n",
      "  -5.15373289e-04 -2.69138578e-04  7.23676288e-04 -9.91257092e-04\n",
      "   0.00000000e+00  0.00000000e+00  4.02823736e-04  6.29263428e-04]]\n",
      "grad0 - [[ 0.00000000e+00 -0.00000000e+00  1.16119320e-03  0.00000000e+00\n",
      "   9.05786122e-05 -0.00000000e+00 -6.42335810e-04  0.00000000e+00\n",
      "   2.96597776e-04  1.44376522e-04  4.91973589e-04 -1.36962433e-04\n",
      "  -0.00000000e+00 -0.00000000e+00  1.39887057e-03  7.30072747e-04]]\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_train_flat, y_train[:1]):\n",
    "    x = np.array([x])\n",
    "    # model.view_weights()\n",
    "    model.train(x, y)\n",
    "    # model.view_neuronValues()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 9, 'confidence': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhurva NU\\AppData\\Local\\Temp\\ipykernel_21772\\3736608636.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-values))\n"
     ]
    }
   ],
   "source": [
    "a = np.array([x_test_flat[0]])\n",
    "print(model.pridict(a,))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
