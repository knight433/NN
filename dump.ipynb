{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        values = np.dot(input_matrix, self.weight_matrix) + self.bias\n",
    "        return self.apply_activation(values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layersT:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons,activation='relu',wig=None,bia=None):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = wig\n",
    "        self.bias = bia\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        values = np.dot(input_matrix, self.weight_matrix) + self.bias\n",
    "        self.neuronValues =  self.apply_activation(values)\n",
    "        #print(f'r =  {self.neuronValues} values = {values}') #debugging\n",
    "        return self.neuronValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        inputNumber = 3 # input nodes\n",
    "        self.numberOfLayers = 3 #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = self.inLayerObj.use(data)\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.array([np.multiply(pridiction,layers.compute_derivative(pridiction))])\n",
    "        prevNuronValues = np.array([self.listOfHiddenLayers[i-1].neuronValues])\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = np.array([layers.compute_derivative(newValues)])\n",
    "            cur_gradient = np.array(np.multiply(derValues.T,pre_w))\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = np.array([self.listOfHiddenLayers[i-1].neuronValues])\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            \n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            print(layers.bias) #debugging\n",
    "\n",
    "            i -= 1\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = np.array([layers.neuronValues])\n",
    "        derValues = np.array(layers.compute_derivative(newValues))\n",
    "        cur_gradient = np.array(np.dot(pre_w,gradient.T))\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        prevNuronValues = np.array([data])\n",
    "        weights = layer.weight_matrix\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wig1 = np.array([[-0.80377147, -0.63344107],\n",
    "                 [0.43278832, -0.02537794],\n",
    "                 [-0.71144554, 0.79602224]])\n",
    "wig2 = np.array([[ 0.30313998,-0.651056],\n",
    "                 [ 0.11990958, -0.68028781]])\n",
    "wig3 = np.array([[-0.13948543, -0.15244189 ,0.02360036],\n",
    "                 [ 0.15260582 ,-0.41243941 ,0.62087499]])\n",
    "\n",
    "b1 = np.array([-0.20960194,-0.86772309])\n",
    "b2 = np.array([-0.04218223, 0.71097538])\n",
    "b3 = np.array([-0.45757658, -0.6612067, 0.39908153])\n",
    "\n",
    "testInputs = [3,2,'relu',wig1,b1,2,2,'relu',wig2,b2,2,3,'softmax',wig3,b3]\n",
    "\n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        inputNumber = 3 # input nodes\n",
    "        self.numberOfLayers = 3 #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "        ind = 0\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = testInputs[ind]\n",
    "            ind += 1\n",
    "            numberOfNeurons = testInputs[ind]\n",
    "            ind += 1\n",
    "            activation = testInputs[ind]\n",
    "            ind += 1\n",
    "            w = testInputs[ind]\n",
    "            ind += 1\n",
    "            b = testInputs[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layersT(number_of_input,numberOfNeurons,activation,w,b)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = self.inLayerObj.use(data)\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label,learningRate=0.1):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "        \n",
    "        # print(costFuntion)\n",
    "        # print(f\"prid - {pridiction}\")\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "        i = self.numberOfLayers - 1\n",
    "\n",
    "        layers = self.listOfHiddenLayers[i]\n",
    "        gradient = np.array([np.multiply(pridiction,layers.compute_derivative(pridiction))])\n",
    "        prevNuronValues = np.array([self.listOfHiddenLayers[i-1].neuronValues])\n",
    "        pr = layers.weight_matrix.T - learningRate*(np.dot(gradient.T,prevNuronValues))\n",
    "        layers.weight_matrix = pr.T\n",
    "        layers.bias = layers.bias - learningRate*(gradient)\n",
    "        print(layers.bias) #debugging\n",
    "        i -= 1\n",
    "\n",
    "        while i:\n",
    "            \n",
    "            #to get gradient \n",
    "            layers = self.listOfHiddenLayers[i]\n",
    "            pre_w = self.listOfHiddenLayers[i+1].weight_matrix\n",
    "            newValues = layers.neuronValues\n",
    "            # print(f'newValues = {i} - {newValues}') #debugging\n",
    "            derValues = np.array([layers.compute_derivative(newValues)])\n",
    "            cur_gradient = np.array(np.multiply(derValues.T,pre_w))\n",
    "            pr = np.dot(gradient,cur_gradient.T)\n",
    "            prevNuronValues = np.array([self.listOfHiddenLayers[i-1].neuronValues])\n",
    "            \n",
    "            #updating weights\n",
    "            weights = layers.weight_matrix\n",
    "            upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "            layers.weight_matrix = upWeights\n",
    "            gradient = pr\n",
    "            i -= 1\n",
    "\n",
    "            #updating the bias \n",
    "            layers.bias = layers.bias - learningRate*(pr)\n",
    "            print(layers.bias) #debugging\n",
    "\n",
    "        #for input layer - first layer value\n",
    "        layer = self.listOfHiddenLayers[0]\n",
    "        pre_w = self.listOfHiddenLayers[1].weight_matrix\n",
    "        newValues = np.array([layers.neuronValues])\n",
    "        #print(f'newvalues =  {newValues}') #debugging\n",
    "        #print(pre_w) #debugging\n",
    "        derValues = np.array(layers.compute_derivative(newValues))\n",
    "        #print(f' der = {derValues}') #debugging\n",
    "        cur_gradient = np.array(np.dot(pre_w,gradient.T))\n",
    "        #print(f'cur-gra = {cur_gradient}') #debugging\n",
    "        pr = np.multiply(derValues,cur_gradient.T)\n",
    "        #print(f'pr = {pr}') #debugging\n",
    "        prevNuronValues = np.array([data])\n",
    "        weights = layer.weight_matrix\n",
    "        #print(weights) #debugging\n",
    "        upWeights = weights.T - learningRate*(np.dot(pr.T,prevNuronValues))\n",
    "        layers.weight_matrix = upWeights.T\n",
    "        print(f'upweights = {upWeights.T}') #debugging\n",
    "        layers.bias = layers.bias - learningRate*(pr)\n",
    "        print(layers.bias) #debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninputs here\\n\\ninput layer = input 3 (change in Model() if req)\\n\\nhidden layer 1 = input - 3, nurons - 2\\nhidden layer 2 = input - 2, nurons - 2\\nhidden layer 2 = input - 2, nurons - 3 (output)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = ModelTest()\n",
    "\n",
    "'''\n",
    "inputs here\n",
    "\n",
    "input layer = input 3 (change in Model() if req)\n",
    "\n",
    "hidden layer 1 = input - 3, nurons - 2\n",
    "hidden layer 2 = input - 2, nurons - 2\n",
    "hidden layer 2 = input - 2, nurons - 3 (output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4486703  -0.66516052  0.3843125 ]]\n",
      "[[-0.04315217  0.70484002]]\n",
      "upweights = [[-0.80479621 -0.6286043 ]\n",
      " [ 0.43073884 -0.0157044 ]\n",
      " [-0.71451976  0.81053254]]\n",
      "[[-0.04417691  0.70967679]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "nn.train(data,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights \n",
      " - [[-0.80377147 -0.63344107]\n",
      " [ 0.43278832 -0.02537794]\n",
      " [-0.71144554  0.79602224]]\n",
      "bias - [-0.20960194 -0.86772309]\n",
      "weights \n",
      " - [[-0.80479621 -0.6286043 ]\n",
      " [ 0.43073884 -0.0157044 ]\n",
      " [-0.71451976  0.81053254]]\n",
      "bias - [[-0.04417691  0.70967679]]\n",
      "weights \n",
      " - [[-0.13896816 -0.15267153  0.02274258]\n",
      " [ 0.15387189 -0.41300147  0.6187755 ]]\n",
      "bias - [[-0.4486703  -0.66516052  0.3843125 ]]\n"
     ]
    }
   ],
   "source": [
    "nn.view_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.30313998  0.11909857]\n",
      " [-0.651056   -0.68541786]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.00811011],\n",
       "       [0.        , 0.05130054]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array([[0,0.836146]])\n",
    "a2 = np.array([[0.00969939, 0.06135357]])\n",
    "\n",
    "b = np.array( [[ 0.30313998 ,-0.651056  ],\n",
    " [ 0.11990958, -0.68028781]])\n",
    "\n",
    "z = np.dot(a2.T,a1)\n",
    "print(b.T-(0.1*z)) #debugging\n",
    "z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
