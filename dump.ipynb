{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer\n",
    "class inputLayer:\n",
    "\n",
    "    def __init__(self, number_of_input): \n",
    "        self.number_of_input = number_of_input\n",
    "    \n",
    "    #to add the vlaues in the network \n",
    "    def use(self, input_matrix):\n",
    "        if isinstance(input_matrix,np.ndarray):\n",
    "            self.out_matrix = input_matrix\n",
    "            return input_matrix\n",
    "        else:\n",
    "            self.out_matrix = np.array(input_matrix)\n",
    "            return self.out_matrix\n",
    "    \n",
    "    def fetchInputValues(self):\n",
    "        return self.out_matrix\n",
    "\n",
    "#hidden and output layers\n",
    "class layers:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu'):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = np.random.uniform(low, high, size=(number_of_input, number_of_neurons))\n",
    "        self.bias = np.random.uniform(low, high, size=(1, number_of_neurons))\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        values = np.dot(input_matrix, self.weight_matrix) + self.bias\n",
    "        return self.apply_activation(values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layersT:\n",
    "    \n",
    "    def __init__(self, number_of_input, number_of_neurons, activation='relu', wig , bia):\n",
    "        low, high = -1, 1\n",
    "        self.weight_matrix = wig\n",
    "        self.bias = bia\n",
    "        self.activation = activation\n",
    "\n",
    "    def apply_activation(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, values)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-values))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(values)\n",
    "        elif self.activation == 'softmax':\n",
    "            e_x = np.exp(values - np.max(values, axis=-1, keepdims=True))\n",
    "            return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "        else:\n",
    "            return values  # No activation (identity)\n",
    "\n",
    "    def compute_derivative(self, values):\n",
    "        if self.activation == 'relu':\n",
    "            return (values > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sigmoid_output = self.apply_activation(values)\n",
    "            return sigmoid_output * (1 - sigmoid_output)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh_output = self.apply_activation(values)\n",
    "            return 1 - tanh_output**2\n",
    "        elif self.activation == 'softmax':\n",
    "            softmax_output = self.apply_activation(values)\n",
    "            return softmax_output * (1 - softmax_output)  # Note: This derivative might not be used often for softmax\n",
    "        else:\n",
    "            return np.ones_like(values)  # Derivative of the identity function is 1\n",
    "\n",
    "    def forward(self, input_matrix):\n",
    "        values = np.dot(input_matrix, self.weight_matrix) + self.bias\n",
    "        return self.apply_activation(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        inputNumber = 3 # input nodes\n",
    "        self.numberOfLayers = 3 #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = int(input('number of input: '))\n",
    "            numberOfNeurons = int(input('number of neurons: '))\n",
    "            activation = input('Enter the activation funtion: ')\n",
    "            hidden_layer_obj = layers(number_of_input,numberOfNeurons,activation)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = self.inLayerObj.use(data)\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "        \n",
    "        # print(reqArr)\n",
    "        # print(costFuntion)\n",
    "        # print(pridiction)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "\n",
    "        for layers in self.listOfHiddenLayers[-1]:\n",
    "            gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "\n",
    "        print(gradient)  \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "wig1 = np.array([[-0.80377147, -0.63344107],\n",
    "                 [0.43278832, -0.02537794],\n",
    "                 [-0.71144554, 0.79602224]])\n",
    "wig2 = np.array([[ 0.30313998,-0.651056],\n",
    "                 [ 0.11990958, -0.68028781]])\n",
    "wig3 = np.array([[-0.13948543, -0.15244189 ,0.02360036],\n",
    "                 [ 0.15260582 ,-0.41243941 ,0.62087499]])\n",
    "\n",
    "b1 = np.array([-0.20960194,-0.86772309])\n",
    "b2 = np.array([-0.04218223, 0.71097538])\n",
    "b3 = np.array([-0.45757658, -0.6612067, 0.39908153])\n",
    "\n",
    "testInputs = [3,2,'relu',wig1,b1,2,2,'relu',wig2,b2,2,3,'softmax',wig3,b3]\n",
    "\n",
    "class ModelTest:\n",
    "\n",
    "    #to intiallize the model\n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        inputNumber = 3 # input nodes\n",
    "        self.numberOfLayers = 3 #number of hidden layer and output layer\n",
    "\n",
    "        self.listOfHiddenLayers = []\n",
    "        self.inLayerObj = inputLayer(inputNumber)\n",
    "        ind = 0\n",
    "        for i in range(self.numberOfLayers):\n",
    "            number_of_input = testInputs[ind]\n",
    "            ind += 1\n",
    "            numberOfNeurons = testInputs[ind]\n",
    "            ind += 1\n",
    "            activation = testInputs[ind]\n",
    "            ind += 1\n",
    "            w = testInputs[ind]\n",
    "            ind += 1\n",
    "            b = testInputs[ind]\n",
    "            ind += 1\n",
    "            hidden_layer_obj = layersT(number_of_input,numberOfNeurons,activation,w,b)\n",
    "\n",
    "            self.listOfHiddenLayers.append(hidden_layer_obj)\n",
    "            \n",
    "            if i == self.numberOfLayers-1:\n",
    "                self.numberOfLabels = numberOfNeurons\n",
    "\n",
    "    def pridict(self,data,ForTraining = False):\n",
    "\n",
    "        arr = self.inLayerObj.use(data)\n",
    "        \n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            arr = layer.forward(arr)\n",
    "\n",
    "        if ForTraining:\n",
    "            return arr\n",
    "\n",
    "        pridiction_value = -1\n",
    "        pridiction = None\n",
    "\n",
    "        for i,obj in enumerate(arr[0]):\n",
    "            if pridiction_value < obj:\n",
    "                pridiction = {'label' : i, 'confidence' : obj}\n",
    "        \n",
    "        return pridiction\n",
    "    \n",
    "\n",
    "    def view_weights(self):\n",
    "        for layer in self.listOfHiddenLayers:\n",
    "            print(f'weights \\n - {layer.weight_matrix}')\n",
    "            print(f'bias - {layer.bias}')\n",
    "\n",
    "    #backpropagation \n",
    "    def train(self,data,label):\n",
    "        \n",
    "        pridiction = self.pridict(data,ForTraining = True)\n",
    "        reqArr =np.array([0 for i in range(self.numberOfLabels)])\n",
    "        reqArr[label] = 1\n",
    "        costFuntion = sum((pridiction - reqArr)**2)\n",
    "        \n",
    "        # print(reqArr)\n",
    "        # print(costFuntion)\n",
    "        # print(pridiction)\n",
    "\n",
    "        pridiction = pridiction - reqArr\n",
    "\n",
    "        for layers in self.listOfHiddenLayers[::-1]:\n",
    "            gradient = np.multiply(pridiction,layers.compute_derivative(pridiction))\n",
    "            print(gradient)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninputs here\\n\\ninput layer = input 3 (change in Model() if req)\\n\\nhidden layer 1 = input - 3, nurons - 2\\nhidden layer 2 = input - 2, nurons - 2\\nhidden layer 2 = input - 2, nurons - 3 (output)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = ModelTest()\n",
    "\n",
    "'''\n",
    "inputs here\n",
    "\n",
    "input layer = input 3 (change in Model() if req)\n",
    "\n",
    "hidden layer 1 = input - 3, nurons - 2\n",
    "hidden layer 2 = input - 2, nurons - 2\n",
    "hidden layer 2 = input - 2, nurons - 3 (output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights \n",
      " - [[-0.12610263  0.12528392]\n",
      " [ 0.31305123  0.94055467]\n",
      " [ 0.11155602  0.63653629]]\n",
      "bias - [[ 0.22930223 -0.28140124]]\n",
      "weights \n",
      " - [[ 0.11912359  0.98243797]\n",
      " [-0.81085228  0.04559269]]\n",
      "bias - [[-0.39662187 -0.13691448]]\n",
      "weights \n",
      " - [[-0.51862473  0.39332542 -0.99474809]\n",
      " [-0.67098109  0.77324672 -0.73810832]]\n",
      "bias - [[-0.94974968  0.86715709 -0.20119708]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nn.view_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08470812  0.21102066  0.01215333]]\n",
      "[[-0.          0.90725851  0.06148264]]\n",
      "[[-0.          0.90725851  0.06148264]]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "nn.train(data,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2, 'confidence': 0.15052100452377867}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_obj = layers(2,3,'softmax')\n",
    "\n",
    "# a = sample_obj.compute_derivative(np.array([0.04001797, 0.02100264, 0.00303844]))\n",
    "# a\n",
    "nn.pridict([4,3,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
